"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.tokenize = void 0;
const constants_1 = require("../constants");
const handlers_1 = require("./handlers");
const contextHandlers = {
    [constants_1.TokenizerContextTypes.Data]: handlers_1.data,
    [constants_1.TokenizerContextTypes.OpenTagStart]: handlers_1.openTagStart,
    [constants_1.TokenizerContextTypes.CloseTag]: handlers_1.closeTag,
    [constants_1.TokenizerContextTypes.Attributes]: handlers_1.attributes,
    [constants_1.TokenizerContextTypes.OpenTagEnd]: handlers_1.openTagEnd,
    [constants_1.TokenizerContextTypes.AttributeKey]: handlers_1.attributeKey,
    [constants_1.TokenizerContextTypes.AttributeValue]: handlers_1.attributeValue,
    [constants_1.TokenizerContextTypes.AttributeValueBare]: handlers_1.attributeValueBare,
    [constants_1.TokenizerContextTypes.AttributeValueWrapped]: handlers_1.attributeValueWrapped,
    [constants_1.TokenizerContextTypes.ScriptContent]: handlers_1.scriptTagContent,
    [constants_1.TokenizerContextTypes.StyleContent]: handlers_1.styleTagContent,
    [constants_1.TokenizerContextTypes.DoctypeOpen]: handlers_1.DoctypeOpen,
    [constants_1.TokenizerContextTypes.DoctypeClose]: handlers_1.DoctypeClose,
    [constants_1.TokenizerContextTypes.DoctypeAttributes]: handlers_1.doctypeAttributes,
    [constants_1.TokenizerContextTypes.DoctypeAttributeWrapped]: handlers_1.doctypeAttributeWrapped,
    [constants_1.TokenizerContextTypes.DoctypeAttributeBare]: handlers_1.doctypeAttributeBare,
    [constants_1.TokenizerContextTypes.CommentContent]: handlers_1.commentContent,
    [constants_1.TokenizerContextTypes.CommentOpen]: handlers_1.noop,
    [constants_1.TokenizerContextTypes.CommentClose]: handlers_1.noop,
};
function tokenizeChars(chars, state, tokens, { isFinalChunk, positionOffset, }) {
    let charIndex = state.caretPosition - positionOffset;
    let charIndexBefore = charIndex;
    while (charIndex < chars.length) {
        const handler = contextHandlers[state.currentContext];
        state.decisionBuffer += chars[charIndex];
        if (charIndexBefore !== charIndex && chars[charIndex] === "\n") {
            state.linePosition++;
        }
        charIndexBefore = charIndex;
        handler.parse(state.decisionBuffer, state, tokens);
        charIndex = state.caretPosition - positionOffset;
    }
    if (isFinalChunk) {
        const handler = contextHandlers[state.currentContext];
        state.caretPosition--;
        if (handler.handleContentEnd !== undefined) {
            handler.handleContentEnd(state, tokens);
        }
    }
}
function tokenize(source = "", { isFinalChunk, } = {}) {
    isFinalChunk = isFinalChunk === undefined ? true : isFinalChunk;
    const state = {
        currentContext: constants_1.TokenizerContextTypes.Data,
        contextParams: {},
        decisionBuffer: "",
        accumulatedContent: "",
        caretPosition: 0,
        linePosition: 1,
        source,
    };
    const chars = state.decisionBuffer + source;
    const tokens = [];
    const positionOffset = state.caretPosition - state.decisionBuffer.length;
    tokenizeChars(chars, state, tokens, {
        isFinalChunk,
        positionOffset,
    });
    return { state, tokens };
}
exports.tokenize = tokenize;
